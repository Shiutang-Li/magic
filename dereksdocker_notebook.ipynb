{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import time\n",
    "import pickle\n",
    "import hashlib\n",
    "import StringIO\n",
    "import urllib, base64\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#from IPython.core.display import HTML\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import xgboost\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn import ensemble\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, average_precision_score, f1_score, precision_score, recall_score, r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "#====================================  SUPPORTING stateless functions  ==============================================#\n",
    "def is_regression(y):\n",
    "    \"\"\"\n",
    "    This function check to see if target is a regression\n",
    "    Params:\n",
    "        y np.array label array\n",
    "    Returns:\n",
    "        True\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y_float = np.array(np.array(y,dtype=int),dtype=float)\n",
    "        return np.sum(np.abs(y_float - y))>0    # Is there an error with int conversion?\n",
    "    except:\n",
    "        return False                            # NOT regression, you throw exception i.e. ['cat','dog']\n",
    "    \n",
    "def hashfile(filename):\n",
    "    \"\"\"\n",
    "    This function will create a hash for a file based on file content\n",
    "    Params:\n",
    "        filename string filename to be hashed\n",
    "    Returns:\n",
    "        string hashed file information\n",
    "    \"\"\"\n",
    "    BLOCKSIZE = 65536\n",
    "    hasher = hashlib.md5()\n",
    "    with open(filename, 'rb') as afile:\n",
    "        buf = afile.read(BLOCKSIZE)\n",
    "        while len(buf) > 0:\n",
    "            hasher.update(buf)\n",
    "            buf = afile.read(BLOCKSIZE)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def export_model(model,results,filename):\n",
    "    \"\"\"\n",
    "    This function will export model and results dictionary to disk for later use\n",
    "    Params:\n",
    "        model class model to be used\n",
    "        basename string model name\n",
    "        filename str filename being input\n",
    "    Returns:\n",
    "        True\n",
    "    \"\"\"\n",
    "    basename = filename.split('.')[0]\n",
    "    # calculate the hash on filename\n",
    "    file_hash = hashfile(filename)\n",
    "    new_filename = basename + '_' + file_hash + '.p'\n",
    "    pickle.dump([model,results],open(new_filename,'wb'))\n",
    "    return\n",
    "     \n",
    "#==============================================  CLASS  ===========================================================#\n",
    "class dereksdocker():\n",
    "    \"\"\"This class will automate supervised regression & classification workflows\"\"\"\n",
    "    \n",
    "    def __init__(self,speed='fast',filename='None'):\n",
    "        nltk.download('wordnet')  #<< make sure this is downloaded\n",
    "        nltk.download('stopwords')\n",
    "        \n",
    "        self.filename = filename\n",
    "        self.df = None\n",
    "        self.speed = speed\n",
    "        self.data_type = None\n",
    "        self.analyzer = CountVectorizer().build_analyzer()\n",
    "        self.lang = \"english\"\n",
    "        self.stops = set(stopwords.words(self.lang))\n",
    "        self.stemmer = SnowballStemmer(self.lang)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.macro_features = None\n",
    "        self.micro_features = None\n",
    "        self.y_label = None\n",
    "        self.results = {}\n",
    "        self.model = None\n",
    "        self.y_pred = None\n",
    "        # run these below to make sure class is ready\n",
    " \n",
    "\n",
    "    def file_2_df(self):\n",
    "        \"\"\"\n",
    "        This function loads any file into a pandass dataframe. \n",
    "        Params:\n",
    "            filename string i.e. file.csv, file.xls\n",
    "        Returns:\n",
    "            df pandas.dataframe \n",
    "        \"\"\" \n",
    "        \n",
    "        # check if csv file\n",
    "        if self.filename.split('.')[-1].lower()=='csv':\n",
    "            print \"csv detected\"\n",
    "            self.df = pd.read_csv(self.filename)\n",
    "            self.data_type='csv'\n",
    "        \n",
    "        #check if tsv file\n",
    "        elif self.filename.split('.')[-1].lower()=='tsv':\n",
    "            print 'text detected'\n",
    "            df = pd.read_csv(self.filename, sep='\\t')\n",
    "            self.df = df[df.columns[::-1]]\n",
    "            self.data_type='text'\n",
    "            print self.df[:2]   #data frame preview\n",
    "        \n",
    "        # else it is an excel file\n",
    "        else:\n",
    "            print \"excel detected\"\n",
    "            self.df = pd.read_excel(self.filename)\n",
    "            self.data_type='excel'\n",
    "        return self.df\n",
    "    \n",
    "    def word_pipeline(self,word):\n",
    "        \"\"\"This function preprocesses the text to get ready for count vectorizer\n",
    "        Params:\n",
    "            word string word to be processed\n",
    "        Returns:\n",
    "            word string processed word\n",
    "        \"\"\"\n",
    "        word = BeautifulSoup(word,\"html5lib\").get_text()       \n",
    "        word = re.sub(\"[^a-zA-Z]\", \" \", word) \n",
    "        word = word.lower() \n",
    "        word = self.stemmer.stem(word)\n",
    "        word = self.lemmatizer.lemmatize(word)\n",
    "        if word in self.stops:\n",
    "            word = None\n",
    "        return word\n",
    "\n",
    "    def process_words(self,doc):\n",
    "        \"\"\"This pipeline calls each word for count vectorizer\"\"\"\n",
    "        return (self.word_pipeline(w) for w in self.analyzer(doc))  \n",
    "    \n",
    "    def df_classifier_input(self):\n",
    "        \"\"\"\n",
    "        This function will take a dataframe and split for binary\n",
    "        Params:\n",
    "            filename string i.e. file.csv, file.xls\n",
    "        Returns:\n",
    "            df pandas.dataframe \n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        macro_features = df.columns[0:-1]     # grab column names\n",
    "        target_label = df.columns[-1]         # grab target column name\n",
    "\n",
    "        # Process input features\n",
    "        if self.data_type=='text':  \n",
    "            \n",
    "            # import tools for text\n",
    "            from sklearn.pipeline import Pipeline\n",
    "            from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "            from scipy.sparse import hstack\n",
    "            from scipy.sparse import csr_matrix\n",
    "\n",
    "            print'This is a text, bag of words model'\n",
    "\n",
    "            def text_processor(df,analyzer='word'):\n",
    "                workflow = Pipeline([('vect', CountVectorizer(analyzer=analyzer)), ('tfidf',TfidfTransformer())])\n",
    "                text_list = list(df[df.columns[0]].values)\n",
    "                X = workflow.fit_transform(text_list)\n",
    "                if analyzer is 'word':\n",
    "                    macro_features = ['text']\n",
    "                else:\n",
    "                    macro_features = ['text_SL']   #SL = stemming & lemming\n",
    "                #/////////////////////////////////\n",
    "                cv = workflow.steps[0][1]\n",
    "                vocab_dict = cv.vocabulary_\n",
    "                id_vocab_dict={}\n",
    "                for key in vocab_dict:\n",
    "                    id_vocab_dict[str(vocab_dict[key])] = key \n",
    "                micro_features =[]\n",
    "                for word in range(0,len(id_vocab_dict)):\n",
    "                    micro_features.append(id_vocab_dict[str(word)])\n",
    "                return X,micro_features,macro_features\n",
    "\n",
    "            def process_sentiment(df):\n",
    "                analyzer = SentimentIntensityAnalyzer()\n",
    "                text_list = list(df[df.columns[0]].values)\n",
    "                data=[]\n",
    "                for msg in text_list:\n",
    "                    vs = analyzer.polarity_scores(msg)\n",
    "                    data.append([vs['neg'],vs['neu'],vs['pos'],vs['compound']])\n",
    "                X = csr_matrix(np.array(data))\n",
    "                micro_features = ['neg_sentiment','neu_sentiment','pos_sentiment','combined_sentiment']\n",
    "                macro_features = ['text_sentiment']\n",
    "                return X,micro_features,macro_features\n",
    "\n",
    "\n",
    "            X,micro_features,macro_features = text_processor(df)\n",
    "            \n",
    "            if self.speed=='slow':\n",
    "                X_2,micro_features_2,macro_features_2 = text_processor(df,analyzer=self.process_words)\n",
    "                X_3,micro_features_3,macro_features_3 = process_sentiment(df)   #calculate sentiment\n",
    "                #Combine all features\n",
    "                micro_features.append(micro_features_2)\n",
    "                micro_features.append(micro_features_3)\n",
    "                macro_features.append(macro_features_2)\n",
    "                macro_features.append(macro_features_3)\n",
    "                X = csr_matrix(hstack((X,X_2,X_3))) # hstacking sparse matrices changes type to COO, changing to CSR\n",
    "\n",
    "            #\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "        else:\n",
    "            print 'NOT text'\n",
    "            X_df_dummy = pd.get_dummies(df[macro_features])     \n",
    "            micro_features = X_df_dummy.columns         # new columns names from tokenizer\n",
    "            X = X_df_dummy.as_matrix()                        # Numpy array, from dataframe\n",
    "\n",
    "        # Process Y\n",
    "        y_df = df[target_label]\n",
    "        y_raw = y_df.as_matrix()\n",
    "\n",
    "        if is_regression(y_raw):\n",
    "            print 'regression detected'\n",
    "            y = y_raw\n",
    "        else:\n",
    "            print 'classification detected'\n",
    "            y_df_dummy = pd.get_dummies(y_df)\n",
    "            y = np.argmax(y_df_dummy.as_matrix(),axis=1) \n",
    "            self.y_label = y_df_dummy.columns\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.macro_features = macro_features\n",
    "        self.micro_features = micro_features\n",
    "        return \n",
    "\n",
    "    # scoring metrics\n",
    "    def scoring(self,y,ypred_class,ypred_prob):\n",
    "        \"\"\"\n",
    "        This function will create a scored dictionary of your predictions\n",
    "        Params:\n",
    "            y np.array actual value\n",
    "            ypred_class np.array predicted values\n",
    "            ypred_prob np.array predicted probabilities of values\n",
    "        Returns:\n",
    "            results dict dictionary of all results \n",
    "        \"\"\"\n",
    "\n",
    "        results={}\n",
    "        if is_regression(y):\n",
    "            results['pearson-r'] = pearsonr(y,ypred_class)[0] \n",
    "            results['rmse'] = mean_squared_error(y, ypred_class)**0.5\n",
    "            results['r2-score'] = r2_score(y,ypred_class)\n",
    "        else:\n",
    "            if len(np.unique(y))>2:\n",
    "                print('Multiclass detected!')\n",
    "                results['accuracy'] = accuracy_score(y,ypred_class)\n",
    "                results['confusion_matrix'] = confusion_matrix(y,ypred_class)\n",
    "                results['observation_count'] = len(y)\n",
    "            else:   # must be binary\n",
    "                print('Binary detected!')\n",
    "                results['auc_score'] = roc_auc_score(y,ypred_prob)\n",
    "                results['accuracy'] = accuracy_score(y,ypred_class)\n",
    "                cm = confusion_matrix(y,ypred_class)\n",
    "                results['confusion_matrix'] = cm\n",
    "                results['pearson-r'] = pearsonr(y,ypred_prob)[0]\n",
    "                results['pearson-r-pval'] = pearsonr(y,ypred_prob)[1]\n",
    "                results['average_precision_score'] = average_precision_score(y,ypred_class)\n",
    "                results['f1_score'] = f1_score(y,ypred_class)\n",
    "                results['precision_score'] = precision_score(y,ypred_class)\n",
    "                results['recall_score'] = recall_score(y,ypred_class)\n",
    "                results['observation_count'] = len(y)\n",
    "                results['label_balance'] = np.mean(y)\n",
    "                results['normalized-confusion-matrix'] = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        results['plots']={}\n",
    "        self.results = results\n",
    "        return self.results\n",
    "    \n",
    "    def train_and_validate(self):    \n",
    "        \"\"\"\n",
    "        This function will train and cross validate your model\n",
    "        Params:\n",
    "            X np.array(matrix) input features\n",
    "            y np.array(vector) target \n",
    "            model class this is the model that will be trained and tested\n",
    "        Returns:\n",
    "            model class trained model object\n",
    "            scoring() function returns scored dictionary\n",
    "        \"\"\"\n",
    "\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "        model = self.model\n",
    "        \n",
    "        #print '*** model = ' , model\n",
    "        ypred_class = np.zeros_like(y,dtype=float)                     # initialize holder array, make sure it is float \n",
    "        ypred_prob = np.zeros_like(y,dtype=float)                      # initialize holder array, make sure it is float \n",
    "           \n",
    "        if is_regression(y): \n",
    "            \n",
    "            kf = KFold(n_splits=10)                    \n",
    "            for train_index, test_index in kf.split(X, y):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                model.fit(X_train,y_train)\n",
    "\n",
    "                ypred_class[test_index] = model.predict(X_test)   \n",
    "             \n",
    "        else: # must be classification\n",
    "            skf = StratifiedKFold(n_splits=10)\n",
    "            for train_index, test_index in skf.split(X, y):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                model.fit(X_train,y_train)\n",
    "\n",
    "                ypred_class[test_index] = model.predict(X_test)\n",
    "                ypred_prob[test_index] = model.predict_proba(X_test)[:,1] \n",
    "                \n",
    "                print('--------------------------------------------')\n",
    "                self.scoring(y,ypred_class,ypred_prob)\n",
    "                print('auc-score = ', self.results['auc_score'])\n",
    "            \n",
    "        self.y_pred = ypred_class\n",
    "        model.fit(X,y)       # training on whole dataset now\n",
    "        self.model = model\n",
    "        return self.model, self.scoring(y,ypred_class,ypred_prob)\n",
    "    \n",
    "    def model_search(self):\n",
    "        \"\"\"\n",
    "        This function will decide what type of model to use\n",
    "        Params:\n",
    "            model class trained model object\n",
    "            results dict trained model scoring information\n",
    "            filename str filename being input\n",
    "        Returns:\n",
    "            True\n",
    "        \"\"\"\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "        \n",
    "        if self.speed=='fast':\n",
    "            if is_regression(y):\n",
    "                self.model = LinearRegression()\n",
    "            else:\n",
    "                self.model = LogisticRegression()\n",
    "        else:   # must be fast\n",
    "            #if self.data_type=='text':   # sparse matrix \n",
    "                \n",
    "            if is_regression(y):\n",
    "                self.model = xgboost.XGBRegressor()\n",
    "            else:\n",
    "                self.model = xgboost.XGBClassifier()\n",
    "            #else:\n",
    "            if is_regression(y):\n",
    "                self.model = ensemble.GradientBoostingRegressor(verbose=True)\n",
    "            else:\n",
    "                self.model = ensemble.GradientBoostingClassifier(verbose=True)                        # define a model\n",
    "\n",
    "            # HYPER PARAM TUNING\n",
    "            # model = gridsearch()\n",
    "            print 'tuning parameters'\n",
    "        #---------------------------------------------------------------------------------------------------#\n",
    "            #param_grid = {\n",
    "            #    'learning_rate' : [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "            #}\n",
    "\n",
    "            #model = GridSearchCV(model, param_grid=param_grid)\n",
    "        #---------------------------------------------------------------------------------------------------#\n",
    "            param_dist = {\n",
    "                'learning_rate' : [0.0001, 0.001, 0.0015, 0.01, 0.1, 0.15, 0.02, 0.2, 0.03, 0.3]\n",
    "            }\n",
    "            #self.model = RandomizedSearchCV(self.model, param_distributions=param_dist)\n",
    "    \n",
    "        return self.model\n",
    "    \n",
    "    def plot_data(self, plotname=None):\n",
    "        if is_regression(self.y):\n",
    "            # Create a Figure\n",
    "            fig = plt.gcf()\n",
    "            # Set up plot\n",
    "            plt.title('Linear Prediction')\n",
    "            plt.xlabel('Actual')\n",
    "            plt.ylabel('Predicted')\n",
    "            plt.scatter(self.y,self.y_pred)\n",
    "            \n",
    "            imgdata = StringIO.StringIO()\n",
    "            fig.savefig(imgdata, format='png')\n",
    "            imgdata.seek(0)  # rewind the data\n",
    "            #print \"Content-type: image/png\\n\"\n",
    "            uri = 'data:image/png;base64,' + urllib.quote(base64.b64encode(imgdata.buf))\n",
    "            #print '<img src = \"%s\"/>' % uri\n",
    "            self.results['plots'][plotname] = uri\n",
    "            plt.close(fig)\n",
    "            \n",
    "        else:\n",
    "            # TO DO\n",
    "            return\n",
    "    def plot_confusion_matrix(self, plotname=None, normalize=False,title='Confusion matrix',cmap=plt.cm.Blues):\n",
    "        \"\"\"\n",
    "        This function prints and plots the confusion matrix.\n",
    "        Normalization can be applied by setting `normalize=True`.\n",
    "        \"\"\"\n",
    "        cm = self.results['confusion_matrix']\n",
    "        #print self.results['plots']\n",
    "        classes = self.y_label\n",
    "        \n",
    "        fig = plt.gcf()\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            #print(\"Normalized confusion matrix\")\n",
    "        else:\n",
    "            pass\n",
    "            #print('Confusion matrix, without normalization')\n",
    "\n",
    "        #print(cm)\n",
    "\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, cm[i, j],\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.grid(False)\n",
    "        \n",
    "        imgdata = StringIO.StringIO()\n",
    "        fig.savefig(imgdata, format='png')\n",
    "        imgdata.seek(0)  # rewind the data\n",
    "        #print \"Content-type: image/png\\n\"\n",
    "        uri = 'data:image/png;base64,' + urllib.quote(base64.b64encode(imgdata.buf))\n",
    "        #print '<img src = \"%s\"/>' % uri\n",
    "        self.results['plots'][plotname] = uri\n",
    "        plt.close(fig)\n",
    "        \n",
    "    def putHTML(self, uri):\n",
    "        source = '<img src = \"%s\"/>' % uri\n",
    "        return HTML(source)    \n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        This function will run entire workflow for prediction from file to model + results\n",
    "        Params:\n",
    "            filename str filename to be run\n",
    "        Returns:\n",
    "            True\n",
    "        \"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.file_2_df()                                    # load file into pandas dataframe\n",
    "        self.df_classifier_input()  # prepare data for ML\n",
    "        self.model_search()                         # model search data \n",
    "\n",
    "        train_model, results = self.train_and_validate()        # train model, cross validate and score\n",
    "        for key in results:\n",
    "            #print key,type(results[key])\n",
    "            if ('numpy.ndarray' in str(type(self.results[key]))) or ('dict' in str(type(self.results[key]))):\n",
    "                #print \"     \",key,\":\", results[key]\n",
    "                pass\n",
    "            else:\n",
    "                if 'plot' in key.lower():\n",
    "                    pass\n",
    "                else:\n",
    "                    print \"     \",key,\":\", round(self.results[key],3)\n",
    "                \n",
    "        # Plot data\n",
    "        if is_regression(self.y):\n",
    "            self.plot_data(plotname='plot1')\n",
    "            #self.putHTML(results['plots'])\n",
    "        else:\n",
    "            self.plot_confusion_matrix(plotname='plot1')\n",
    "            self.plot_confusion_matrix(plotname='plot2',normalize=True)\n",
    "            #self.putHTML(self.results['plots'])\n",
    "        \n",
    "        export_model(train_model,results, self.filename)                 # save model to disk\n",
    "        run_time = time.time() - start_time\n",
    "        print \"Ran in %.3f seconds\" % run_time\n",
    "        return self.results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
